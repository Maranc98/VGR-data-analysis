{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "from llama_cpp import llama_log_set\n",
    "\n",
    "def my_log_callback(level, message, user_data):\n",
    "    pass\n",
    "\n",
    "log_callback = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)(my_log_callback)\n",
    "llama_log_set(log_callback, ctypes.c_void_p())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_path = \"./llama-2-7b-chat.Q5_K_M.gguf\"\n",
    "llm = Llama(model_path=llm_path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow_datasets as tfds\n",
    "# Prompt evaluation\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "\n",
    "base_path = \"./data/prompt_search/\"\n",
    "\n",
    "#prompts = get_candidate_prompts(meta_prompt, n=10)\n",
    "\n",
    "prompts = [\n",
    "    \"What is the emotion of the following text?\",\n",
    "    \"List the emotions of the following text\",\n",
    "    \"Say what the emotions in the following text are\",\n",
    "    \"What are the emotions of the following text?\",\n",
    "    \"Which are the emotions present in the following text?\",\n",
    "    \"Write a list of all the emotions in the following text\",\n",
    "    \"Write all the emotions in the following text, include synonyms which are nouns and be repetitive\",\n",
    "    \"Write all nouns describing the emotions in the following text\",\n",
    "    \"Write all the emotions present in the following text. You can only use the following words: \" + \", \".join(emotions),\n",
    "    \"Write all the emotions present in the following text. Do not write emotions that are not present. You can only use the following words: \" + \", \".join(emotions),\n",
    "]\n",
    "\n",
    "prompt_id = 0\n",
    "prompt_results = {\n",
    "    \"prompt_id\": [],\n",
    "    \"prompt\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall\": [],\n",
    "    \"precision\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"resampling_iteration\": [],\n",
    "    \"ds_training_samples\": [],\n",
    "}\n",
    "\n",
    "resampling_iterations = 10\n",
    "prompts_per_resampling = 1\n",
    "training_samples = 100\n",
    "for i in range(resampling_iterations):\n",
    "    \n",
    "    # On the new samples \n",
    "    for prompt in prompts:\n",
    "        print(f\"Evaluating prompt \\\"{prompt}\\\" on {training_samples} training samples of the go emotions dataset\")\n",
    "        test_ds = tfds.load('huggingface:go_emotions/simplified', split=f'train[:{training_samples}]')\n",
    "        num_items, num_labels = len(test_ds), len(emotions)\n",
    "\n",
    "        accuracy_list = []\n",
    "        recall_list = []\n",
    "        \n",
    "        ds = test_ds.as_numpy_iterator()\n",
    "        y_gt_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        inference_examples = {\n",
    "            \"prompt_id\": [],\n",
    "            \"prompt\": [],\n",
    "            \"text\": [],\n",
    "            \"output\": [],\n",
    "            \"gt_emotions\": [],\n",
    "            \"pred_emotions\": [],\n",
    "        }\n",
    "\n",
    "        for j in tqdm(range(num_items)):\n",
    "            # Get next example\n",
    "            x = ds.next()\n",
    "            x_txt = x[\"text\"].decode('utf-8')\n",
    "            \n",
    "            # Compute llm output and predictions\n",
    "            eval_prompt = \"Q: \" + prompt + \" Text: \" + x_txt + \" A:\"\n",
    "            output = llm(eval_prompt, max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "            gt_emotions = [emotions[i] for i in x[\"labels\"]]\n",
    "            pred_emotions = [em for em in emotions if em in output['choices'][0]['text'].lower()]\n",
    "            \n",
    "            #if j == 3:\n",
    "            #    break\n",
    "            \n",
    "            if j < 5:\n",
    "                # Print LLM outputs and predictions\n",
    "                print(\"Prompt:\\t\", eval_prompt)\n",
    "                print(\"Answer:\\t\", output['choices'][0]['text'])\n",
    "                print(\"GT:\\t\", \" \".join(gt_emotions))\n",
    "                print(\"Y:\\t\", \" \".join(pred_emotions))\n",
    "\n",
    "                # Save inference examples\n",
    "                inference_examples[\"prompt_id\"].append(prompt_id)\n",
    "                inference_examples[\"prompt\"].append(prompt)\n",
    "                inference_examples[\"text\"].append(x_txt)\n",
    "                inference_examples[\"output\"].append(output['choices'][0]['text'])\n",
    "                inference_examples[\"gt_emotions\"].append(gt_emotions)\n",
    "                inference_examples[\"pred_emotions\"].append(pred_emotions)\n",
    "\n",
    "            # Compute ground truth and predictions as one-hot vectors\n",
    "            y_gt = np.zeros(num_labels)\n",
    "            for i in x[\"labels\"]:\n",
    "                y_gt[i] = 1\n",
    "            y_gt_list.append(y_gt)\n",
    "\n",
    "            y_pred_idxs = [i for i in range(len(emotions)) if emotions[i] in pred_emotions]\n",
    "            y_pred = np.zeros(num_labels)\n",
    "            for i in y_pred_idxs:\n",
    "                y_pred[i] = 1\n",
    "            y_pred_list.append(y_pred)\n",
    "\n",
    "        y_gt = np.array(y_gt_list)\n",
    "        y_pred = np.array(y_pred_list)\n",
    "\n",
    "        # Save prompt predictions and ground truth\n",
    "        np.save(f\"{base_path}/y_gt_{prompt_id}.npy\", y_gt)\n",
    "        np.save(f\"{base_path}/y_pred_{prompt_id}.npy\", y_pred)\n",
    "\n",
    "        # Compute prompt accuracy and recall\n",
    "        accuracy = np.sum(y_gt == y_pred) / np.prod(y_gt.shape)\n",
    "        recall = np.sum((y_gt == y_pred) & (y_gt == 1)) / np.sum(y_gt == 1)\n",
    "        precision = np.sum((y_gt == y_pred) & (y_pred == 1)) / np.sum(y_pred == 1)\n",
    "        f1 = 2 / (1/recall + 1/precision)\n",
    "        print(f\"> Accuracy: {accuracy*100:.2f}%\")\n",
    "        print(f\"> Recall: {recall*100:.2f}%\")\n",
    "        print(f\"> Precision: {precision*100:.2f}%\\n\")\n",
    "        print(f\"> F1 score: {f1*100:.2f}%\\n\")\n",
    "        \n",
    "        # Add prompt scores to results\n",
    "        prompt_results[\"prompt_id\"].append(prompt_id)\n",
    "        prompt_results[\"prompt\"].append(prompt)\n",
    "        prompt_results[\"accuracy\"].append(accuracy)\n",
    "        prompt_results[\"recall\"].append(recall)\n",
    "        prompt_results[\"precision\"].append(precision)\n",
    "        prompt_results[\"f1_score\"].append(f1)\n",
    "        prompt_results[\"resampling_iteration\"].append(i)\n",
    "        prompt_results[\"ds_training_samples\"].append(training_samples)\n",
    "\n",
    "        # Save prompt inference examples\n",
    "        df = pd.DataFrame(inference_examples)\n",
    "        df.to_csv(f\"{base_path}/inference_examples_{prompt_id}.csv\", index=False)\n",
    "        prompt_id += 1\n",
    "\n",
    "    # Resample prompts\n",
    "    old_prompts = prompt_results[\"prompt\"]\n",
    "    old_scores = prompt_results[\"f1_score\"]\n",
    "    sampling_p = np.array(old_scores) / np.sum(old_scores)\n",
    "    \n",
    "    prompts = []\n",
    "    for _ in range(prompts_per_resampling):\n",
    "        prompt_to_resample = np.random.choice(old_prompts, p=sampling_p)\n",
    "        output = llm(\"Q: Rewrite the following text while preserving its meaning. Text: \" + prompt_to_resample + \"A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "        print(output['choices'][0]['text'])\n",
    "        prompts.append(output['choices'][0]['text'])\n",
    "        \n",
    "# Save results for all prompts\n",
    "df = pd.DataFrame(prompt_results)\n",
    "print(df)\n",
    "df.to_csv(f\"{base_path}/prompt_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "base_path = \"./data/prompt_search/\"\n",
    "prompt_results = pd.read_csv(f\"{base_path}/prompt_results.csv\")\n",
    "#print(prompt_results)\n",
    "prompt_results = prompt_results.sort_values(by=\"f1_score\", ascending=False)\n",
    "print(prompt_results[\"prompt\"].values[0])\n",
    "\n",
    "display_prompt_results = prompt_results.drop([\"resampling_iteration\", \"ds_training_samples\", \"prompt_id\"], axis=1)\n",
    "display_prompt_results = display_prompt_results.reset_index(drop=True)\n",
    "latex = display_prompt_results.round(3).to_latex(index=False, formatters={\"name\": str.upper}, float_format=\"{:.3f}\".format)\n",
    "with open(f\"data/prompt_search/f1_prompts.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "results_hm = display_prompt_results.style.background_gradient(cmap='Blues', subset=[\"accuracy\", \"recall\", \"precision\", \"f1_score\"])\n",
    "results_hm.hide(axis='index')\n",
    "display(results_hm)\n",
    "\n",
    "with open(f\"data/prompt_search/results_by_f1.html\", \"w\") as f:\n",
    "    f.write(results_hm.to_html())\n",
    "\n",
    "display_prompt_results = prompt_results.sort_values(by=\"recall\", ascending=False)\n",
    "print(display_prompt_results[\"prompt\"].values[0])\n",
    "display_prompt_results = display_prompt_results.drop([\"resampling_iteration\", \"ds_training_samples\", \"prompt_id\"], axis=1)\n",
    "display_prompt_results = display_prompt_results.reset_index(drop=True)\n",
    "\n",
    "latex = display_prompt_results.round(3).to_latex(index=False, formatters={\"name\": str.upper}, float_format=\"{:.3f}\".format)\n",
    "with open(f\"data/prompt_search/recall_prompts.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "results_hm = display_prompt_results.style.background_gradient(cmap='Blues', subset=[\"accuracy\", \"recall\", \"precision\", \"f1_score\"])\n",
    "results_hm.hide(axis='index')\n",
    "display(results_hm)\n",
    "\n",
    "with open(f\"data/prompt_search/results_by_recall.html\", \"w\") as f:\n",
    "    f.write(results_hm.to_html())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "studied_emotions = ['anger', 'annoyance', 'disgust', 'fear', 'grief', 'joy', 'love', 'neutral', 'optimism', 'pride', 'relief', 'sadness', 'surprise']\n",
    "\n",
    "if \"chat\" in llm_path:\n",
    "    meta_prompt_0 = \"Q: A relationship connects these input-output pairs: \\n\"\n",
    "    meta_prompt_1 = \"What can i ask a person to give me those outputs when presented with these inputs? \"\n",
    "    meta_prompt_1 += \"Only state one question. A: \"\n",
    "else:\n",
    "    meta_prompt_0 = \"Q: I gave a person an instruction and some inputs. \" \n",
    "    meta_prompt_0 += \"The person read the instruction and wrote an output for every one of the inputs. \"\n",
    "    meta_prompt_0 += \"Here are the input-output pairs: \\n\"\n",
    "    meta_prompt_1 = \"The instruction was A:\"\n",
    "\n",
    "def get_prompt_examples():\n",
    "    train_ds = tfds.load('huggingface:go_emotions/simplified', split='train')\n",
    "    num_items, num_labels = len(train_ds), len(emotions)\n",
    "    # Turn prefetch dataset into itreable dataset\n",
    "    train_ds = train_ds.as_numpy_iterator()\n",
    "\n",
    "    examples = []\n",
    "    emotions_used = set()\n",
    "    for i in tqdm(range(num_items)):\n",
    "        x = train_ds.next()\n",
    "\n",
    "        extracted_emotions = [i for i in x[\"labels\"] if emotions[i] in studied_emotions]\n",
    "\n",
    "        is_old_emotion = [i in emotions_used for i in extracted_emotions]\n",
    "        if all(is_old_emotion):\n",
    "            continue\n",
    "\n",
    "        x_txt = x[\"text\"].decode('utf-8')\n",
    "\n",
    "        y_txt = \" \".join([emotions[i] for i in extracted_emotions])\n",
    "\n",
    "        for i in extracted_emotions:\n",
    "            emotions_used.add(i)\n",
    "        \n",
    "        examples.append(f\"IN: {x_txt} OUT: {y_txt}\")\n",
    "        #print(x[\"text\"].decode('utf-8'), y_txt)\n",
    "        \n",
    "        #if len(emotions_used) == len(studied_emotions):\n",
    "        #    break\n",
    "        if len(examples) > 5:\n",
    "            break\n",
    "    \n",
    "    return examples\n",
    "\n",
    "ex = get_prompt_examples()\n",
    "print(ex)\n",
    "print(len(ex))\n",
    "meta_prompt = meta_prompt_0 + \"\\n\".join(ex) + \"\\n\" + meta_prompt_1\n",
    "print(meta_prompt)\n",
    "print(len(meta_prompt.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_prompts(prompt, n=500):\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        output = llm(\"Q: \" + prompt + \"A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "        print(output['choices'][0]['text'])\n",
    "        prompts.append(output['choices'][0]['text'])\n",
    "    return prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
