{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Go emotions ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_ds = tfds.load('huggingface:go_emotions/simplified', split='test')\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "\n",
    "num_items, num_labels = len(train_ds), len(emotions)\n",
    "train_ds = train_ds.as_numpy_iterator()\n",
    "\n",
    "y_targets = np.zeros((num_items, num_labels), dtype=int)\n",
    "for i in tqdm(range(num_items)):\n",
    "    x = train_ds.next()\n",
    "    if i < 5:\n",
    "        print(x['text'])\n",
    "    labels = x['labels']\n",
    "    for j in labels:\n",
    "        y_targets[i, j] = 1\n",
    "\n",
    "np.save(\"data/model_eval/y_targets.npy\", y_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute roberta predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None, max_length=512, truncation=True)\n",
    "train_ds = tfds.load('huggingface:go_emotions/simplified', split='test')\n",
    "\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "opt_thresholds = [0.25, 0.45, 0.15, 0.1, 0.3, 0.4, 0.55, 0.25, 0.25, 0.4, 0.3, 0.2, 0.1, 0.35, 0.4, 0.45, 0.05, 0.4, 0.25, 0.25, 0.2, 0.1, 0.15, 0.05, 0.1, 0.4, 0.15, 0.25]\n",
    "\n",
    "num_items, num_labels = len(train_ds), len(emotions)\n",
    "train_ds_iter = train_ds.as_numpy_iterator()\n",
    "\n",
    "def get_detected_emotions(model_output, thresholds=0.5):\n",
    "    th = np.asarray(thresholds)\n",
    "    emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "    outputs = {d['label']: d['score'] for d in model_output[0]}\n",
    "    output_list = [outputs[em] for em in emotions]\n",
    "    emotion_ids = np.where(np.asarray(output_list) > th)[0]\n",
    "    emotions = np.asarray(emotions)[emotion_ids]\n",
    "    return emotion_ids, emotions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bert_pred = np.zeros((num_items, num_labels), dtype=int)\n",
    "for i in tqdm(range(num_items)):\n",
    "    x = train_ds_iter.next()\n",
    "    if i < 5:\n",
    "        print(x['text'])\n",
    "    labels = classifier(x['text'].decode('utf-8'))[0]\n",
    "    labels, _ = get_detected_emotions([labels], thresholds=opt_thresholds)\n",
    "    for j in labels:\n",
    "        y_bert_pred[i, j] = 1\n",
    "\n",
    "np.save(\"data/model_eval/y_bert_pred_optth.npy\", y_bert_pred)\n",
    "\n",
    "train_ds_iter = train_ds.as_numpy_iterator()\n",
    "y_bert_pred = np.zeros((num_items, num_labels), dtype=int)\n",
    "for i in tqdm(range(num_items)):\n",
    "    x = train_ds_iter.next()\n",
    "    if i < 5:\n",
    "        print(x['text'])\n",
    "    labels = classifier(x['text'].decode('utf-8'))[0]\n",
    "    labels, _ = get_detected_emotions([labels], thresholds=0.5)\n",
    "    for j in labels:\n",
    "        y_bert_pred[i, j] = 1\n",
    "\n",
    "np.save(\"data/model_eval/y_bert_pred_05.npy\", y_bert_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LLM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "from llama_cpp import llama_log_set\n",
    "\n",
    "def my_log_callback(level, message, user_data):\n",
    "    pass\n",
    "\n",
    "log_callback = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)(my_log_callback)\n",
    "llama_log_set(log_callback, ctypes.c_void_p())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "test_ds = tfds.load('huggingface:go_emotions/simplified', split='test')\n",
    "num_items, num_labels = len(test_ds), len(emotions)\n",
    "\n",
    "path = \"./llama-2-7b-chat.Q5_K_M.gguf\"\n",
    "llm = Llama(model_path=path, verbose=False)\n",
    "\n",
    "high_recall_prompt = \"Write all the emotions present in the following text. Do not write emotions that are not present. You can only use the following words: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, neutral, optimism, pride, realization, relief, remorse, sadness, surprise\"\n",
    "high_f1_prompt = \"Which are the emotions present in the following text?\"\n",
    "\n",
    "def get_llm_predictions(text, prompt):\n",
    "    eval_prompt = \"Q: \" + prompt + \" Text: \" + text + \" A:\"\n",
    "    output = llm(eval_prompt, max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    pred_emotions_idx = [i for i, em in enumerate(emotions) if em in output['choices'][0]['text'].lower()]\n",
    "    y = np.zeros(len(emotions), dtype=int)\n",
    "    y[pred_emotions_idx] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High f1 score prompt evaluation\n",
    "#y_llm_pred = np.zeros((num_items, num_labels), dtype=int)\n",
    "#test_ds_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "if False:\n",
    "    for i in tqdm(range(num_items)):\n",
    "        x = test_ds_iter.next()\n",
    "        text = x['text'].decode('utf-8')\n",
    "        labels = get_llm_predictions(text, high_f1_prompt)\n",
    "        y_llm_pred[i, :] = labels\n",
    "        \n",
    "        if i < 5:\n",
    "            print(x['text'])\n",
    "            print(labels)\n",
    "\n",
    "    np.save(\"data/model_eval/y_llm_f1.npy\", y_llm_pred)\n",
    "\n",
    "# High recall prompt evaluation\n",
    "#y_llm_pred = np.zeros((num_items, num_labels), dtype=int)\n",
    "i0 = 600 + 3100 + 700\n",
    "y_llm_pred = np.load(\"data/model_eval/y_llm_recall.npy\")\n",
    "test_ds_iter = test_ds.as_numpy_iterator()\n",
    "for _ in range(i0+1):\n",
    "    x = test_ds_iter.next()\n",
    "\n",
    "#for i in tqdm(range(num_items)):\n",
    "for i in tqdm(range(i0+1, num_items)):\n",
    "    x = test_ds_iter.next()\n",
    "    text = x['text'].decode('utf-8')\n",
    "    labels = get_llm_predictions(text, high_recall_prompt)\n",
    "    y_llm_pred[i, :] = labels\n",
    "    \n",
    "    if i < 5:\n",
    "        print(x['text'])\n",
    "        print(labels)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        np.save(\"data/model_eval/y_llm_recall.npy\", y_llm_pred)\n",
    "\n",
    "np.save(\"data/model_eval/y_llm_recall.npy\", y_llm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tables with metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "def compare_models(y_gt, y_pred, threshold=0.5):\n",
    "    emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
    "    results = []\n",
    "    for label_index, label in enumerate(emotions):\n",
    "        y_targets, y_preds = y_gt[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"ROBERTA_0.5\", \"data/model_eval/y_bert_pred_05.npy\", \"roberta_05\"),\n",
    "    (\"ROBERTA_opt\", \"data/model_eval/y_bert_pred_optth.npy\", \"roberta_opt\"),\n",
    "    (\"LLM_recall\", \"data/model_eval/y_llm_recall.npy\", \"llm_recall\"),\n",
    "    (\"LLM_f1\", \"data/model_eval/y_llm_f1.npy\", \"llm_f1\"),\n",
    "]\n",
    "\n",
    "mean_scores_log = []\n",
    "for model_name, pred_path, savefile_name in models:\n",
    "\n",
    "    # Compute results\n",
    "    y_gt = np.load(\"data/model_eval/y_targets.npy\")\n",
    "    y_preds = np.load(pred_path)\n",
    "    results = compare_models(y_gt, y_preds)\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=emotions)\n",
    "    per_label_results = per_label_results.drop(columns=[\"label\"])\n",
    "    display(per_label_results.round(3))\n",
    "\n",
    "    # Save model mean scores\n",
    "    mean_scores = per_label_results.mean(axis=0)\n",
    "    mean_scores_log.append(mean_scores)\n",
    "    mean_scores.to_csv(f\"data/model_eval/mean_scores_{savefile_name}.csv\")\n",
    "    print(mean_scores)\n",
    "\n",
    "    # Save latex table\n",
    "    latex = per_label_results.round(3).to_latex(index=False, formatters={\"name\": str.upper}, float_format=\"{:.1f}\".format)\n",
    "    with open(f\"data/model_eval/metrics_per_label_{savefile_name}.tex\", \"w\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    # Save html table with colors\n",
    "    results_hm = per_label_results.style.background_gradient(cmap='Blues', subset=[\"accuracy\", \"recall\", \"precision\", \"f1\", \"mcc\"])\n",
    "    display(results_hm)\n",
    "\n",
    "    with open(f\"data/prompt_search/metrics_per_label_{savefile_name}.html\", \"w\") as f:\n",
    "        f.write(results_hm.to_html())\n",
    "\n",
    "# Save mean scores and tables\n",
    "model_names = [m[0] for m in models]\n",
    "mean_scores_df = pd.DataFrame(mean_scores_log, index=model_names)\n",
    "mean_scores_df = mean_scores_df.drop(columns=[\"support\", \"threshold\"])\n",
    "display(mean_scores_df.round(3))\n",
    "\n",
    "# Save latex table\n",
    "latex = mean_scores_df.round(3).to_latex(index=True, formatters={\"name\": str.upper}, float_format=\"{:.1f}\".format)\n",
    "with open(f\"data/model_eval/mean_scores.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "# Save html table with colors\n",
    "results_hm = mean_scores_df.style.background_gradient(cmap='Blues', subset=[\"accuracy\", \"recall\", \"precision\", \"f1\", \"mcc\"])\n",
    "display(results_hm)\n",
    "\n",
    "with open(f\"data/model_eval/mean_scores.html\", \"w\") as f:\n",
    "    f.write(results_hm.to_html())\n",
    "\n",
    "latex = mean_scores_df.round(3).to_latex(index=True, formatters={\"name\": str.upper}, float_format=\"{:.3f}\".format)\n",
    "with open(f\"data/model_eval/mean_scores.tex\", \"w\") as f:\n",
    "    f.write(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = np.load(\"data/model_eval/y_targets.npy\")[:]\n",
    "y_preds = np.load(\"data/model_eval/y_bert_pred_optth.npy\")[:]\n",
    "print(y_gt.shape, y_preds.shape)\n",
    "results = compare_models(y_gt, y_preds)\n",
    "\n",
    "per_label_results = pd.DataFrame(results, index=emotions)\n",
    "per_label_results = per_label_results.drop(columns=[\"label\"])\n",
    "display(per_label_results.round(3))\n",
    "\n",
    "# Save model mean scores\n",
    "mean_scores = per_label_results.mean(axis=0)\n",
    "print(mean_scores)\n",
    "mean_scores.to_csv(\"data/model_eval/mean_scores_roberta_optth.csv\")\n",
    "\n",
    "# Save results in display format\n",
    "latex = per_label_results.round(3).to_latex(index=False, formatters={\"name\": str.upper}, float_format=\"{:.1f}\".format)\n",
    "with open(\"data/model_eval/metrics_per_label_roberta.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "results_hm = per_label_results.style.background_gradient(cmap='Blues', subset=[\"accuracy\", \"recall\", \"precision\", \"f1\", \"mcc\"])\n",
    "display(results_hm)\n",
    "\n",
    "with open(f\"data/prompt_search/metrics_per_label_roberta.html\", \"w\") as f:\n",
    "    f.write(results_hm.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Run bert predictions\n",
    "- Implement LLM prompt search\n",
    "- Run it for results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
